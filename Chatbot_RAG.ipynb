{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **\"Generative AI ChatBot\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to make ChatBots using pretrained models and RAG.\n",
    "\n",
    "#### **Libraries Used:**\n",
    "- gradio\n",
    "- PyPDF2\n",
    "- faiss\n",
    "- numpy\n",
    "- sentence_transformers\n",
    "- google.generativeai\n",
    "- openai\n",
    "- GTTS\n",
    "- temp_file\n",
    "- speech_recognition\n",
    "\n",
    "#### **Work Flow:**\n",
    "- Load model using API key\n",
    "- Read PDF\n",
    "- Make Chunks of the text extracted from PDF\n",
    "- Encode passages using Sentence Transformers\n",
    "- Create FAISS index\n",
    "- Retrieve passages based on input query\n",
    "- Generate answers based on retrieved passages\n",
    "- Mention the source of the response of the bot (from which PDF it took the response and from which page)\n",
    "- Host it on Gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The workflow for applying RAG is as follows:\n",
    "\n",
    "1. Read PDF\n",
    "2. Make Chunks of the text extracted from PDF (this is done to reduce the size of the model input tokens and it will be able to read large PDFs)\n",
    "3. Encode passages using Sentence Transformers (this is done because we have to convert text to vectors)\n",
    "4. Create FAISS index (this is done to search the passages in the index)\n",
    "5. Retrieve passages based on input query\n",
    "6. Generate answers based on retrieved passages\n",
    "7. Mention the source of the response of the bot (from which PDF it took the response and from which page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Majid Hanif\\.conda\\envs\\myenv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Majid Hanif\\.conda\\envs\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['API_KEY'] =\"Add Your Key\"  # Replace with your actual API key\n",
    "genai.configure(api_key=os.environ['API_KEY'])\n",
    "\n",
    "# Choose a model\n",
    "gen_model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "pdf_data = []  # To store passages and their metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Multiple PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdfs(pdf_files):\n",
    "    all_texts = []\n",
    "    for pdf_file in pdf_files:\n",
    "        text = \"\"\n",
    "        with open(pdf_file.name, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        pdf_name = os.path.basename(pdf_file.name)\n",
    "        all_texts.append((text, pdf_name))\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(text, pdf_name, chunk_size=500, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    page_numbers = []\n",
    "    start = 0\n",
    "    page = 1  # Start with page 1\n",
    "    while start < len(text):\n",
    "        chunk = text[start:start + chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "            page_numbers.append((pdf_name, page))\n",
    "        start += chunk_size - chunk_overlap\n",
    "        if start % chunk_size == 0:  # Move to next page roughly after chunk size\n",
    "            page += 1\n",
    "    return chunks, page_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encode passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Majid Hanif\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def encode_passages(passages):\n",
    "    embeddings = sentence_model.encode(passages, convert_to_tensor=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings.cpu().numpy())\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieve passages based on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_passages(index, passages, page_numbers, query, k=5):\n",
    "    query_embedding = sentence_model.encode(query, convert_to_tensor=True)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    relevant_passages = []\n",
    "    relevant_page_numbers = []\n",
    "\n",
    "    for i in range(k):\n",
    "        if indices[0][i] < len(passages) and distances[0][i] < 1.3:  # Threshold for relevance\n",
    "            relevant_passages.append(passages[indices[0][i]])\n",
    "            relevant_page_numbers.append(page_numbers[indices[0][i]])\n",
    "\n",
    "    return list(zip(relevant_passages, relevant_page_numbers)), distances[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate answer based on retrieved passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(gen_model, prompt, retrieved_passages):\n",
    "    response = \"\"\n",
    "    sources_info = {}\n",
    "\n",
    "    # Collect passages by source\n",
    "    for passage, (pdf, page) in retrieved_passages:\n",
    "        if (pdf, page) not in sources_info:\n",
    "            sources_info[(pdf, page)] = []\n",
    "        sources_info[(pdf, page)].append(passage)\n",
    "\n",
    "    # Construct the response\n",
    "    for (pdf, page), passages in sources_info.items():\n",
    "        # Join all passages from the same source and page\n",
    "        response += \"\\n\".join(passages) + f\" [Source: {pdf}, Page: {page}]\\n\\n\"\n",
    "\n",
    "    # Generate the final response text\n",
    "    response_text = gen_model.generate_content(prompt + \"\\n\\n\" + response)\n",
    "\n",
    "    # Prepare sources list\n",
    "    unique_sources = set(sources_info.keys())\n",
    "    sources_list = \"\\n\".join([f\"[Source: {pdf}, Page: {page}]\" for pdf, page in unique_sources])\n",
    "\n",
    "    return response_text.text + \"\\n\\nSources:\\n\" + sources_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Define chatbot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(prompt, state, pdf_files):\n",
    "    global indexes, pdf_data  # Declare global variables\n",
    "    pdf_data = []  # Reset for new input\n",
    "    indexes = []\n",
    "\n",
    "    # Read and process the PDFs\n",
    "    all_texts = read_pdfs(pdf_files)\n",
    "    for text, pdf_name in all_texts:\n",
    "        passages, page_numbers = make_chunks(text, pdf_name)\n",
    "        embeddings = encode_passages(passages)\n",
    "        index = create_index(embeddings)\n",
    "        indexes.append((index, passages, page_numbers))\n",
    "\n",
    "    # Retrieve relevant passages from all PDFs\n",
    "    retrieved_passages = []\n",
    "    for index, passages, page_numbers in indexes:\n",
    "        passages_batch, distances = retrieve_passages(index, passages, page_numbers, prompt)\n",
    "\n",
    "        # Debugging output\n",
    "        print(f\"Distances: {distances}\")\n",
    "        \n",
    "        # Check if any retrieved passages have a low distance (indicating relevance)\n",
    "        if len(distances) > 0 and np.any(distances < 1.3):  # Adjust threshold as needed\n",
    "            retrieved_passages.extend(passages_batch)\n",
    "\n",
    "    # Generate response based on the retrieved passages\n",
    "    if not retrieved_passages:\n",
    "        response = \"I don't have this information. For more information, contact +123456789.\" # If info asked is out of PDF\n",
    "    else:\n",
    "        response = generate_answer(gen_model, prompt, retrieved_passages)\n",
    "\n",
    "    return response, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://9efe4097e9f256d3f1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9efe4097e9f256d3f1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [1.2891536 1.4580312 1.6631095 1.6664965 1.7959213]\n"
     ]
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=chatbot,\n",
    "    inputs=[\"text\", \"state\", gr.File(label=\"Upload PDFs\", file_count=\"multiple\")],\n",
    "    outputs=[\"text\", \"state\"],\n",
    "    title=\"PDF Chatbot with RAG\",\n",
    "    description=\"Ask me anything based on the uploaded PDFs!\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
